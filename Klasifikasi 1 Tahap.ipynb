{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9e1b53f-e1ef-4bf5-8cd5-0a5c7afcbf6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>dominant_label</th>\n",
       "      <th>binary_label</th>\n",
       "      <th>tokenized_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>its.mamia</td>\n",
       "      <td>Udah rusak penampilan, agama. Minimal akhlak lah</td>\n",
       "      <td>['Flaming']</td>\n",
       "      <td>Yes</td>\n",
       "      <td>['sudah', 'rusak', 'tampil', 'agama', 'minimum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sendhikautama</td>\n",
       "      <td>Lu yg mana bang</td>\n",
       "      <td>['Neutral']</td>\n",
       "      <td>No</td>\n",
       "      <td>['bang']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>herru_dic</td>\n",
       "      <td>SOK IYEE bngt baru satu lagu yg booming hadehhh</td>\n",
       "      <td>['Flaming']</td>\n",
       "      <td>Yes</td>\n",
       "      <td>['megah', 'iyee', 'banget', 'lagu', 'booming',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dfitrangga</td>\n",
       "      <td>Wkwkwkw iya betul lagiâ€¦</td>\n",
       "      <td>['Neutral']</td>\n",
       "      <td>No</td>\n",
       "      <td>['wkwkwkw', 'ya']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_grestisaa</td>\n",
       "      <td>Pril gak suka malah badannya kecil kepalanya g...</td>\n",
       "      <td>['Body Shaming']</td>\n",
       "      <td>Yes</td>\n",
       "      <td>['pril', 'tidak', 'suka', 'tepat', 'badan', 'k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10307</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Saya tidak ingin ingin ... Anda sangat bangga ðŸ˜Œ</td>\n",
       "      <td>['Flaming', 'Harassment']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['tidak', 'bangga']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10308</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Tidak perlu membuat lagu, Love Langwrede Grand...</td>\n",
       "      <td>['Flaming', 'Harassment']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['tidak', 'butuh', 'lagu', 'cinta', 'langwrede...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10309</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Tidak ada yang mengalahkan orang ... sudah san...</td>\n",
       "      <td>['Flaming', 'Harassment']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['tidak', 'kalah', 'orang', 'sudah', 'senar', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10310</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Saya siap mendengar lagu baru Anda Arogan, mes...</td>\n",
       "      <td>['Flaming', 'Harassment']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['dengar', 'lagu', 'arogan', 'b']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10311</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Saya baru saja mendengarnya dan tidak menyukai...</td>\n",
       "      <td>['Flaming', 'Harassment']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['dengar', 'tidak', 'suka', 'pikir', 'lagu', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10312 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            username                                               text  \\\n",
       "0          its.mamia   Udah rusak penampilan, agama. Minimal akhlak lah   \n",
       "1      sendhikautama                                    Lu yg mana bang   \n",
       "2          herru_dic    SOK IYEE bngt baru satu lagu yg booming hadehhh   \n",
       "3         dfitrangga                            Wkwkwkw iya betul lagiâ€¦   \n",
       "4         _grestisaa  Pril gak suka malah badannya kecil kepalanya g...   \n",
       "...              ...                                                ...   \n",
       "10307            NaN    Saya tidak ingin ingin ... Anda sangat bangga ðŸ˜Œ   \n",
       "10308            NaN  Tidak perlu membuat lagu, Love Langwrede Grand...   \n",
       "10309            NaN  Tidak ada yang mengalahkan orang ... sudah san...   \n",
       "10310            NaN  Saya siap mendengar lagu baru Anda Arogan, mes...   \n",
       "10311            NaN  Saya baru saja mendengarnya dan tidak menyukai...   \n",
       "\n",
       "                  dominant_label binary_label  \\\n",
       "0                    ['Flaming']          Yes   \n",
       "1                    ['Neutral']           No   \n",
       "2                    ['Flaming']          Yes   \n",
       "3                    ['Neutral']           No   \n",
       "4               ['Body Shaming']          Yes   \n",
       "...                          ...          ...   \n",
       "10307  ['Flaming', 'Harassment']          NaN   \n",
       "10308  ['Flaming', 'Harassment']          NaN   \n",
       "10309  ['Flaming', 'Harassment']          NaN   \n",
       "10310  ['Flaming', 'Harassment']          NaN   \n",
       "10311  ['Flaming', 'Harassment']          NaN   \n",
       "\n",
       "                                       tokenized_comment  \n",
       "0      ['sudah', 'rusak', 'tampil', 'agama', 'minimum...  \n",
       "1                                               ['bang']  \n",
       "2      ['megah', 'iyee', 'banget', 'lagu', 'booming',...  \n",
       "3                                      ['wkwkwkw', 'ya']  \n",
       "4      ['pril', 'tidak', 'suka', 'tepat', 'badan', 'k...  \n",
       "...                                                  ...  \n",
       "10307                                ['tidak', 'bangga']  \n",
       "10308  ['tidak', 'butuh', 'lagu', 'cinta', 'langwrede...  \n",
       "10309  ['tidak', 'kalah', 'orang', 'sudah', 'senar', ...  \n",
       "10310                  ['dengar', 'lagu', 'arogan', 'b']  \n",
       "10311  ['dengar', 'tidak', 'suka', 'pikir', 'lagu', '...  \n",
       "\n",
       "[10312 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(r'C:\\Users\\towik\\Downloads\\praproses_klasifikasi1.csv')\n",
    "df2 = pd.read_csv(r'C:\\Users\\towik\\Downloads\\praproses_klasifikasi2.csv')\n",
    "df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd80eefd-04b1-415b-9d94-194249265540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>dominant_label</th>\n",
       "      <th>binary_label</th>\n",
       "      <th>tokenized_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>its.mamia</td>\n",
       "      <td>Udah rusak penampilan, agama. Minimal akhlak lah</td>\n",
       "      <td>['Flaming']</td>\n",
       "      <td>Yes</td>\n",
       "      <td>['sudah', 'rusak', 'tampil', 'agama', 'minimum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sendhikautama</td>\n",
       "      <td>Lu yg mana bang</td>\n",
       "      <td>['Neutral']</td>\n",
       "      <td>No</td>\n",
       "      <td>['bang']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>herru_dic</td>\n",
       "      <td>SOK IYEE bngt baru satu lagu yg booming hadehhh</td>\n",
       "      <td>['Flaming']</td>\n",
       "      <td>Yes</td>\n",
       "      <td>['megah', 'iyee', 'banget', 'lagu', 'booming',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dfitrangga</td>\n",
       "      <td>Wkwkwkw iya betul lagiâ€¦</td>\n",
       "      <td>['Neutral']</td>\n",
       "      <td>No</td>\n",
       "      <td>['wkwkwkw', 'ya']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_grestisaa</td>\n",
       "      <td>Pril gak suka malah badannya kecil kepalanya g...</td>\n",
       "      <td>['Body Shaming']</td>\n",
       "      <td>Yes</td>\n",
       "      <td>['pril', 'tidak', 'suka', 'tepat', 'badan', 'k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7525</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Saya tidak ingin ingin ... Anda sangat bangga ðŸ˜Œ</td>\n",
       "      <td>['Flaming', 'Harassment']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['tidak', 'bangga']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7526</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Tidak perlu membuat lagu, Love Langwrede Grand...</td>\n",
       "      <td>['Flaming', 'Harassment']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['tidak', 'butuh', 'lagu', 'cinta', 'langwrede...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7527</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Tidak ada yang mengalahkan orang ... sudah san...</td>\n",
       "      <td>['Flaming', 'Harassment']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['tidak', 'kalah', 'orang', 'sudah', 'senar', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7528</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Saya siap mendengar lagu baru Anda Arogan, mes...</td>\n",
       "      <td>['Flaming', 'Harassment']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['dengar', 'lagu', 'arogan', 'b']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7529</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Saya baru saja mendengarnya dan tidak menyukai...</td>\n",
       "      <td>['Flaming', 'Harassment']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['dengar', 'tidak', 'suka', 'pikir', 'lagu', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7530 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           username                                               text  \\\n",
       "0         its.mamia   Udah rusak penampilan, agama. Minimal akhlak lah   \n",
       "1     sendhikautama                                    Lu yg mana bang   \n",
       "2         herru_dic    SOK IYEE bngt baru satu lagu yg booming hadehhh   \n",
       "3        dfitrangga                            Wkwkwkw iya betul lagiâ€¦   \n",
       "4        _grestisaa  Pril gak suka malah badannya kecil kepalanya g...   \n",
       "...             ...                                                ...   \n",
       "7525            NaN    Saya tidak ingin ingin ... Anda sangat bangga ðŸ˜Œ   \n",
       "7526            NaN  Tidak perlu membuat lagu, Love Langwrede Grand...   \n",
       "7527            NaN  Tidak ada yang mengalahkan orang ... sudah san...   \n",
       "7528            NaN  Saya siap mendengar lagu baru Anda Arogan, mes...   \n",
       "7529            NaN  Saya baru saja mendengarnya dan tidak menyukai...   \n",
       "\n",
       "                 dominant_label binary_label  \\\n",
       "0                   ['Flaming']          Yes   \n",
       "1                   ['Neutral']           No   \n",
       "2                   ['Flaming']          Yes   \n",
       "3                   ['Neutral']           No   \n",
       "4              ['Body Shaming']          Yes   \n",
       "...                         ...          ...   \n",
       "7525  ['Flaming', 'Harassment']          NaN   \n",
       "7526  ['Flaming', 'Harassment']          NaN   \n",
       "7527  ['Flaming', 'Harassment']          NaN   \n",
       "7528  ['Flaming', 'Harassment']          NaN   \n",
       "7529  ['Flaming', 'Harassment']          NaN   \n",
       "\n",
       "                                      tokenized_comment  \n",
       "0     ['sudah', 'rusak', 'tampil', 'agama', 'minimum...  \n",
       "1                                              ['bang']  \n",
       "2     ['megah', 'iyee', 'banget', 'lagu', 'booming',...  \n",
       "3                                     ['wkwkwkw', 'ya']  \n",
       "4     ['pril', 'tidak', 'suka', 'tepat', 'badan', 'k...  \n",
       "...                                                 ...  \n",
       "7525                                ['tidak', 'bangga']  \n",
       "7526  ['tidak', 'butuh', 'lagu', 'cinta', 'langwrede...  \n",
       "7527  ['tidak', 'kalah', 'orang', 'sudah', 'senar', ...  \n",
       "7528                  ['dengar', 'lagu', 'arogan', 'b']  \n",
       "7529  ['dengar', 'tidak', 'suka', 'pikir', 'lagu', '...  \n",
       "\n",
       "[7530 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined = df_combined.drop_duplicates(subset=['text'], keep='first').reset_index(drop=True)\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beb5ed02-bc84-4b49-9456-ce7367fe843d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dominant_label\n",
       "['Neutral']                        2958\n",
       "['Denigration']                     930\n",
       "['Harassment']                      669\n",
       "['Body Shaming']                    475\n",
       "['Flaming']                         409\n",
       "['Denigration', 'Flaming']          395\n",
       "['Body Shaming', 'Denigration']     387\n",
       "['Flaming', 'Harassment']           375\n",
       "['Body Shaming', 'Harassment']      319\n",
       "['Denigration', 'Harassment']       312\n",
       "['Body Shaming', 'Flaming']         301\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined['dominant_label'].apply(str).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0673a919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "['Neutral']                        2958\n",
      "['Denigration']                     930\n",
      "['Harassment']                      669\n",
      "['Body Shaming']                    475\n",
      "['Flaming']                         409\n",
      "['Denigration', 'Flaming']          395\n",
      "['Body Shaming', 'Denigration']     387\n",
      "['Flaming', 'Harassment']           375\n",
      "['Body Shaming', 'Harassment']      319\n",
      "['Denigration', 'Harassment']       312\n",
      "['Body Shaming', 'Flaming']         301\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "data = df_combined\n",
    "data.rename(columns={'dominant_label': 'label'}, inplace=True)\n",
    "\n",
    "# Konversi label dari string ke list (jika perlu)\n",
    "data['label'] = data['label'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Urutkan label di tempatnya agar formatnya seragam\n",
    "data['label'] = data['label'].apply(lambda x: sorted(x))\n",
    "\n",
    "# Hitung distribusi dengan memastikan list diubah menjadi format yang bisa dihitung\n",
    "label_distribution = data['label'].apply(str).value_counts()\n",
    "\n",
    "# Tampilkan hasil distribusi\n",
    "print(label_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6927dcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label yang telah dikonversi ke numerik:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body Shaming</th>\n",
       "      <th>Denigration</th>\n",
       "      <th>Flaming</th>\n",
       "      <th>Harassment</th>\n",
       "      <th>Neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Body Shaming  Denigration  Flaming  Harassment  Neutral\n",
       "0             0            0        1           0        0\n",
       "1             0            0        0           0        1\n",
       "2             0            0        1           0        0\n",
       "3             0            0        0           0        1\n",
       "4             1            0        0           0        0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Load dataset\n",
    "# data = pd.read_csv('/content/drive/MyDrive/Dataset/Skripsi/preprocessed_data.csv')\n",
    "# data.rename(columns={'dominant_label': 'label'}, inplace=True)\n",
    "\n",
    "# Konversi label dari string ke list (jika masih dalam format string)\n",
    "# import ast\n",
    "# data['label'] = data['label'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Inisialisasi MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Konversi label ke format biner\n",
    "y = mlb.fit_transform(data['label'])\n",
    "\n",
    "# Buat DataFrame untuk label yang sudah dikonversi\n",
    "label_df = pd.DataFrame(y, columns=mlb.classes_)\n",
    "\n",
    "# Gabungkan dengan dataset awal (Opsional)\n",
    "data = pd.concat([data, label_df], axis=1)\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(\"Label yang telah dikonversi ke numerik:\")\n",
    "label_df.head()\n",
    "\n",
    "# Simpan hasil ke file CSV (Opsional)\n",
    "# data.to_csv('/content/drive/MyDrive/Dataset/Skripsi/data_numerik_label.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82e8c1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout 80:20 -> Training: 6024, Testing: 1506\n",
      "Holdout 70:30 -> Training: 5271, Testing: 2259\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data['tokenized_comment']\n",
    "y = data[['Body Shaming', 'Denigration', 'Flaming', 'Harassment', 'Neutral']]\n",
    "# Holdout 80:20\n",
    "X_train_80, X_test_20, y_train_80, y_test_20 = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# Holdout 70:30\n",
    "X_train_70, X_test_30, y_train_70, y_test_30 = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Cek ukuran dataset setelah split\n",
    "print(f\"Holdout 80:20 -> Training: {X_train_80.shape[0]}, Testing: {X_test_20.shape[0]}\")\n",
    "print(f\"Holdout 70:30 -> Training: {X_train_70.shape[0]}, Testing: {X_test_30.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2586082b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "[Neutral]                      2958\n",
       "[Denigration]                   930\n",
       "[Harassment]                    669\n",
       "[Body Shaming]                  475\n",
       "[Flaming]                       409\n",
       "[Denigration, Flaming]          395\n",
       "[Body Shaming, Denigration]     387\n",
       "[Flaming, Harassment]           375\n",
       "[Body Shaming, Harassment]      319\n",
       "[Denigration, Harassment]       312\n",
       "[Body Shaming, Flaming]         301\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d41245b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Unigram 80:20 -> Train Shape: (6024, 4655)\n",
      "TF-IDF Unigram 80:20 -> Test Shape: (1506, 4655)\n",
      "TF-IDF Bigram 80:20 -> Train Shape: (6024, 21273)\n",
      "TF-IDF Bigram 80:20 -> Test Shape: (1506, 21273)\n",
      "TF-IDF Trigram 80:20 -> Train Shape: (6024, 24294)\n",
      "TF-IDF Trigram 80:20 -> Test Shape: (1506, 24294)\n",
      "\n",
      "TF-IDF Unigram 70:30 -> Train Shape: (5271, 4335)\n",
      "TF-IDF Unigram 70:30 -> Test Shape: (2259, 4335)\n",
      "TF-IDF Bigram 70:30 -> Train Shape: (5271, 19134)\n",
      "TF-IDF Bigram 70:30 -> Test Shape: (2259, 19134)\n",
      "TF-IDF Trigram 70:30 -> Train Shape: (5271, 21483)\n",
      "TF-IDF Trigram 70:30 -> Test Shape: (2259, 21483)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_uni = TfidfVectorizer(ngram_range=(1,1))  # Unigram\n",
    "vectorizer_bi = TfidfVectorizer(ngram_range=(2,2))  # Bigram\n",
    "vectorizer_tri = TfidfVectorizer(ngram_range=(3,3))  # Trigram\n",
    "# TF-IDF 80:20\n",
    "X_train_80_tfidf_uni = vectorizer_uni.fit_transform(X_train_80)\n",
    "X_test_20_tfidf_uni = vectorizer_uni.transform(X_test_20)\n",
    "X_train_80_tfidf_bi = vectorizer_bi.fit_transform(X_train_80)\n",
    "X_test_20_tfidf_bi = vectorizer_bi.transform(X_test_20)\n",
    "X_train_80_tfidf_tri = vectorizer_tri.fit_transform(X_train_80)\n",
    "X_test_20_tfidf_tri = vectorizer_tri.transform(X_test_20)\n",
    "# TF-IDF 70:30\n",
    "X_train_70_tfidf_uni = vectorizer_uni.fit_transform(X_train_70)\n",
    "X_test_30_tfidf_uni = vectorizer_uni.transform(X_test_30)\n",
    "X_train_70_tfidf_bi = vectorizer_bi.fit_transform(X_train_70)\n",
    "X_test_30_tfidf_bi = vectorizer_bi.transform(X_test_30)\n",
    "X_train_70_tfidf_tri = vectorizer_tri.fit_transform(X_train_70)\n",
    "X_test_30_tfidf_tri = vectorizer_tri.transform(X_test_30)\n",
    "\n",
    "# Cek ukuran fitur setelah TF-IDF\n",
    "print(f\"TF-IDF Unigram 80:20 -> Train Shape: {X_train_80_tfidf_uni.shape}\")\n",
    "print(f\"TF-IDF Unigram 80:20 -> Test Shape: {X_test_20_tfidf_uni.shape}\")\n",
    "print(f\"TF-IDF Bigram 80:20 -> Train Shape: {X_train_80_tfidf_bi.shape}\")\n",
    "print(f\"TF-IDF Bigram 80:20 -> Test Shape: {X_test_20_tfidf_bi.shape}\")\n",
    "print(f\"TF-IDF Trigram 80:20 -> Train Shape: {X_train_80_tfidf_tri.shape}\")\n",
    "print(f\"TF-IDF Trigram 80:20 -> Test Shape: {X_test_20_tfidf_tri.shape}\")\n",
    "\n",
    "print(f\"\\nTF-IDF Unigram 70:30 -> Train Shape: {X_train_70_tfidf_uni.shape}\")\n",
    "print(f\"TF-IDF Unigram 70:30 -> Test Shape: {X_test_30_tfidf_uni.shape}\")\n",
    "print(f\"TF-IDF Bigram 70:30 -> Train Shape: {X_train_70_tfidf_bi.shape}\")\n",
    "print(f\"TF-IDF Bigram 70:30 -> Test Shape: {X_test_30_tfidf_bi.shape}\")\n",
    "print(f\"TF-IDF Trigram 70:30 -> Train Shape: {X_train_70_tfidf_tri.shape}\")\n",
    "print(f\"TF-IDF Trigram 70:30 -> Test Shape: {X_test_30_tfidf_tri.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a6857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall scikit-multilearn -y\n",
    "# !pip install scikit-multilearn==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efc06e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall scikit-learn\n",
    "# !pip install scikit-learn==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f652c3-ec90-440d-9909-9f59145879ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Label Powerset ===\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, precision_recall_fscore_support, classification_report\n",
    "\n",
    "classifiers = {\n",
    "    'SVM': SVC(kernel='linear', probability=True),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100)\n",
    "}\n",
    "\n",
    "print(\"=== Label Powerset ===\")\n",
    "for name, model in classifiers.items():\n",
    "    clf = LabelPowerset(classifier=model)\n",
    "    clf.fit(X_train_70_tfidf_uni, y_train_70)\n",
    "    y_pred = clf.predict(X_test_30_tfidf_uni)\n",
    "\n",
    "    print(f\"\\n=== LP-{name} 70:30 Unigram ===\")\n",
    "\n",
    "    subset_acc = accuracy_score(y_test_30, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test_30, y_pred, average='macro')\n",
    "    h_loss = hamming_loss(y_test_30, y_pred)\n",
    "\n",
    "    print(f\"Subset Accuracy: {subset_acc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "    print(f\"Hamming Loss: {h_loss:.4f}\")\n",
    "    print(classification_report(y_test_30, y_pred, target_names=y.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19821af-c8c5-43c3-8c14-32afb4a34baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "\n",
    "classifiers = {\n",
    "    'SVM': SVC(kernel='linear', probability=True),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100)\n",
    "}\n",
    "\n",
    "print(\"=== Classifier Chains ===\")\n",
    "for name, model in classifiers.items():\n",
    "    clf = ClassifierChain(classifier=model)\n",
    "    clf.fit(X_train_70_tfidf_uni, y_train_70)\n",
    "    y_pred = clf.predict(X_test_30_tfidf_uni)\n",
    "\n",
    "    print(f\"\\n=== CC-{name} 70:30 Unigram ===\")\n",
    "\n",
    "    subset_acc = accuracy_score(y_test_30, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test_30, y_pred, average='macro')\n",
    "    h_loss = hamming_loss(y_test_30, y_pred)\n",
    "\n",
    "    print(f\"Subset Accuracy: {subset_acc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "    print(f\"Hamming Loss: {h_loss:.4f}\")\n",
    "    print(classification_report(y_test_30, y_pred, target_names=y.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87639383-c107-43d2-8ca8-5c5a365d2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, precision_recall_fscore_support, classification_report\n",
    "\n",
    "# Daftar algoritma\n",
    "classifiers = {\n",
    "    'SVM': SVC(kernel='linear', probability=True),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100)\n",
    "}\n",
    "\n",
    "print(\"=== Binary Relevance ===\")\n",
    "for name, model in classifiers.items():\n",
    "    clf = BinaryRelevance(classifier=model)\n",
    "    clf.fit(X_train_70_tfidf_uni, y_train_70)\n",
    "    y_pred = clf.predict(X_test_30_tfidf_uni)\n",
    "\n",
    "    print(f\"\\n=== BR-{name} 70:30 Unigram ===\")\n",
    "\n",
    "    # Metrics\n",
    "    subset_acc = accuracy_score(y_test_30, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test_30, y_pred, average='macro')\n",
    "    h_loss = hamming_loss(y_test_30, y_pred)\n",
    "\n",
    "    print(f\"Subset Accuracy: {subset_acc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "    print(f\"Hamming Loss: {h_loss:.4f}\")\n",
    "    print(classification_report(y_test_30, y_pred, target_names=y.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f7bd6b-88a9-4bae-9ddf-7214f158cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall scikit-learn\n",
    "# !pip install scikit-learn==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a7206d-46a6-4ae6-a20b-07daa02e0a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall scikit-learn\n",
    "# !pip install scikit-learn==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca68a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "lp_knn = LabelPowerset(KNeighborsClassifier(n_neighbors=5))\n",
    "from skmultilearn.adapt import MLkNN\n",
    "ml_knn = MLkNN(k=5)\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, hamming_loss, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f2b5ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LP-KNN 80:20 Unigram ===\n",
      "Subset Accuracy: 0.6262\n",
      "Precision: 0.7583, Recall: 0.6708, F1-score: 0.6788\n",
      "Hamming Loss: 0.1602\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Body Shaming       0.94      0.65      0.77       296\n",
      " Denigration       0.77      0.52      0.62       404\n",
      "     Flaming       0.85      0.52      0.65       296\n",
      "  Harassment       0.86      0.55      0.67       335\n",
      "     Neutral       0.55      0.93      0.69       592\n",
      "\n",
      "   micro avg       0.69      0.67      0.68      1923\n",
      "   macro avg       0.80      0.63      0.68      1923\n",
      "weighted avg       0.76      0.67      0.68      1923\n",
      " samples avg       0.64      0.65      0.64      1923\n",
      "\n",
      "\n",
      "=== LP-KNN 80:20 Bigram ===\n",
      "Subset Accuracy: 0.3406\n",
      "Precision: 0.6339, Recall: 0.5902, F1-score: 0.4976\n",
      "Hamming Loss: 0.3110\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Body Shaming       0.93      0.54      0.69       296\n",
      " Denigration       0.35      0.95      0.51       404\n",
      "     Flaming       0.94      0.49      0.64       296\n",
      "  Harassment       0.29      0.91      0.44       335\n",
      "     Neutral       0.73      0.24      0.36       592\n",
      "\n",
      "   micro avg       0.42      0.59      0.49      1923\n",
      "   macro avg       0.65      0.63      0.53      1923\n",
      "weighted avg       0.63      0.59      0.50      1923\n",
      " samples avg       0.45      0.53      0.47      1923\n",
      "\n",
      "\n",
      "=== LP-KNN 80:20 Trigram ===\n",
      "Subset Accuracy: 0.4874\n",
      "Precision: 0.7434, Recall: 0.4659, F1-score: 0.4443\n",
      "Hamming Loss: 0.2351\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Body Shaming       1.00      0.31      0.47       296\n",
      " Denigration       0.62      0.30      0.41       404\n",
      "     Flaming       0.98      0.17      0.29       296\n",
      "  Harassment       0.95      0.16      0.28       335\n",
      "     Neutral       0.47      0.98      0.63       592\n",
      "\n",
      "   micro avg       0.55      0.47      0.50      1923\n",
      "   macro avg       0.80      0.38      0.41      1923\n",
      "weighted avg       0.74      0.47      0.44      1923\n",
      " samples avg       0.51      0.50      0.50      1923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training LP-KNN pada TF-IDF Unigram 80:20\n",
    "lp_knn.fit(X_train_80_tfidf_uni, y_train_80)\n",
    "y_pred_lp_knn_20 = lp_knn.predict(X_test_20_tfidf_uni)\n",
    "\n",
    "# Evaluasi LP-KNN (80:20)\n",
    "print(\"\\n=== LP-KNN 80:20 Unigram ===\")\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_20, y_pred_lp_knn_20, average='weighted', zero_division=0)\n",
    "hamming_loss_lp_knn_20 = hamming_loss(y_test_20, y_pred_lp_knn_20)\n",
    "subset_accuracy = accuracy_score(y_test_20, y_pred_lp_knn_20)\n",
    "print(f\"Subset Accuracy: {subset_accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming_loss_lp_knn_20:.4f}\")\n",
    "print(classification_report(y_test_20, y_pred_lp_knn_20, target_names=mlb.classes_, zero_division=0))\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Training LP-KNN pada TF-IDF Bigram 80:20\n",
    "lp_knn.fit(X_train_80_tfidf_bi, y_train_80)\n",
    "y_pred_lp_knn_20 = lp_knn.predict(X_test_20_tfidf_bi)\n",
    "\n",
    "# Evaluasi LP-KNN (80:20)\n",
    "print(\"\\n=== LP-KNN 80:20 Bigram ===\")\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_20, y_pred_lp_knn_20, average='weighted', zero_division=0)\n",
    "hamming_loss_lp_knn_20 = hamming_loss(y_test_20, y_pred_lp_knn_20)\n",
    "subset_accuracy = accuracy_score(y_test_20, y_pred_lp_knn_20)\n",
    "print(f\"Subset Accuracy: {subset_accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming_loss_lp_knn_20:.4f}\")\n",
    "print(classification_report(y_test_20, y_pred_lp_knn_20, target_names=mlb.classes_, zero_division=0))\n",
    "\n",
    "# Training LP-KNN pada TF-IDF Trigram 80:20\n",
    "lp_knn.fit(X_train_80_tfidf_tri, y_train_80)\n",
    "y_pred_lp_knn_20 = lp_knn.predict(X_test_20_tfidf_tri)\n",
    "\n",
    "# Evaluasi LP-KNN (80:20)\n",
    "print(\"\\n=== LP-KNN 80:20 Trigram ===\")\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_20, y_pred_lp_knn_20, average='weighted', zero_division=0)\n",
    "hamming_loss_lp_knn_20 = hamming_loss(y_test_20, y_pred_lp_knn_20)\n",
    "subset_accuracy = accuracy_score(y_test_20, y_pred_lp_knn_20)\n",
    "print(f\"Subset Accuracy: {subset_accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming_loss_lp_knn_20:.4f}\")\n",
    "print(classification_report(y_test_20, y_pred_lp_knn_20, target_names=mlb.classes_, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0c9b30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LP-KNN 70:30 Unigram ===\n",
      "Subset Accuracy: 0.6082\n",
      "Precision: 0.7371, Recall: 0.6558, F1-score: 0.6668\n",
      "Hamming Loss: 0.1658\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Body Shaming       0.93      0.61      0.74       445\n",
      " Denigration       0.68      0.56      0.61       607\n",
      "     Flaming       0.85      0.54      0.66       443\n",
      "  Harassment       0.85      0.51      0.64       503\n",
      "     Neutral       0.56      0.88      0.69       887\n",
      "\n",
      "   micro avg       0.68      0.66      0.67      2885\n",
      "   macro avg       0.77      0.62      0.67      2885\n",
      "weighted avg       0.74      0.66      0.67      2885\n",
      " samples avg       0.63      0.63      0.63      2885\n",
      "\n",
      "\n",
      "=== LP-KNN 70:30 Bigram ===\n",
      "Subset Accuracy: 0.5219\n",
      "Precision: 0.8008, Recall: 0.5026, F1-score: 0.4957\n",
      "Hamming Loss: 0.2223\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Body Shaming       0.97      0.33      0.50       445\n",
      " Denigration       0.96      0.28      0.43       607\n",
      "     Flaming       0.97      0.27      0.42       443\n",
      "  Harassment       0.92      0.26      0.40       503\n",
      "     Neutral       0.46      0.99      0.63       887\n",
      "\n",
      "   micro avg       0.57      0.50      0.54      2885\n",
      "   macro avg       0.86      0.43      0.48      2885\n",
      "weighted avg       0.80      0.50      0.50      2885\n",
      " samples avg       0.53      0.53      0.53      2885\n",
      "\n",
      "\n",
      "=== LP-KNN 70:30 Trigram ===\n",
      "Subset Accuracy: 0.4706\n",
      "Precision: 0.7950, Recall: 0.4225, F1-score: 0.3819\n",
      "Hamming Loss: 0.2533\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Body Shaming       1.00      0.21      0.34       445\n",
      " Denigration       0.96      0.16      0.27       607\n",
      "     Flaming       0.96      0.16      0.27       443\n",
      "  Harassment       0.92      0.16      0.27       503\n",
      "     Neutral       0.43      1.00      0.60       887\n",
      "\n",
      "   micro avg       0.50      0.42      0.46      2885\n",
      "   macro avg       0.85      0.33      0.35      2885\n",
      "weighted avg       0.79      0.42      0.38      2885\n",
      " samples avg       0.47      0.47      0.47      2885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training LP-KNN pada TF-IDF Unigram 70:30\n",
    "lp_knn.fit(X_train_70_tfidf_uni, y_train_70)\n",
    "y_pred_lp_knn_30 = lp_knn.predict(X_test_30_tfidf_uni)\n",
    "\n",
    "# Evaluasi LP-KNN (70:30)\n",
    "print(\"\\n=== LP-KNN 70:30 Unigram ===\")\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_30, y_pred_lp_knn_30, average='weighted', zero_division=0)\n",
    "hamming_loss_lp_knn_30 = hamming_loss(y_test_30, y_pred_lp_knn_30)\n",
    "subset_accuracy = accuracy_score(y_test_30, y_pred_lp_knn_30)\n",
    "print(f\"Subset Accuracy: {subset_accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming_loss_lp_knn_30:.4f}\")\n",
    "print(classification_report(y_test_30, y_pred_lp_knn_30, target_names=mlb.classes_, zero_division=0))\n",
    "\n",
    "\n",
    "# Training LP-KNN pada TF-IDF Bigram 70:30\n",
    "lp_knn.fit(X_train_70_tfidf_bi, y_train_70)\n",
    "y_pred_lp_knn_30 = lp_knn.predict(X_test_30_tfidf_bi)\n",
    "\n",
    "# Evaluasi LP-KNN (70:30)\n",
    "print(\"\\n=== LP-KNN 70:30 Bigram ===\")\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_30, y_pred_lp_knn_30, average='weighted', zero_division=0)\n",
    "hamming_loss_lp_knn_30 = hamming_loss(y_test_30, y_pred_lp_knn_30)\n",
    "subset_accuracy = accuracy_score(y_test_30, y_pred_lp_knn_30)\n",
    "print(f\"Subset Accuracy: {subset_accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming_loss_lp_knn_30:.4f}\")\n",
    "print(classification_report(y_test_30, y_pred_lp_knn_30, target_names=mlb.classes_, zero_division=0))\n",
    "\n",
    "# Training LP-KNN pada TF-IDF Trigram 70:30\n",
    "lp_knn.fit(X_train_70_tfidf_tri, y_train_70)\n",
    "y_pred_lp_knn_30 = lp_knn.predict(X_test_30_tfidf_tri)\n",
    "\n",
    "# Evaluasi LP-KNN (70:30)\n",
    "print(\"\\n=== LP-KNN 70:30 Trigram ===\")\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_30, y_pred_lp_knn_30, average='weighted', zero_division=0)\n",
    "hamming_loss_lp_knn_30 = hamming_loss(y_test_30, y_pred_lp_knn_30)\n",
    "subset_accuracy = accuracy_score(y_test_30, y_pred_lp_knn_30)\n",
    "print(f\"Subset Accuracy: {subset_accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming_loss_lp_knn_30:.4f}\")\n",
    "print(classification_report(y_test_30, y_pred_lp_knn_30, target_names=mlb.classes_, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0d29b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.adapt import MLkNN\n",
    "from sklearn.metrics import precision_recall_fscore_support, hamming_loss, accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Inisialisasi ML-KNN dengan K=\n",
    "ml_knn = MLkNN(k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "007062d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ML-KNN 80:20 Unigram ===\n",
      "Subset Accuracy: 0.5876\n",
      "Precision: 0.7365, Recall: 0.7098, F1-score: 0.6897\n",
      "Hamming Loss: 0.1604\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Body Shaming       0.78      0.78      0.78       296\n",
      " Denigration       0.88      0.50      0.63       404\n",
      "     Flaming       0.69      0.70      0.69       296\n",
      "  Harassment       0.90      0.54      0.67       335\n",
      "     Neutral       0.55      0.93      0.69       592\n",
      "\n",
      "   micro avg       0.68      0.71      0.69      1923\n",
      "   macro avg       0.76      0.69      0.69      1923\n",
      "weighted avg       0.74      0.71      0.69      1923\n",
      " samples avg       0.65      0.68      0.66      1923\n",
      "\n",
      "\n",
      "=== ML-KNN 80:20 Bigram ===\n",
      "Subset Accuracy: 0.2510\n",
      "Precision: 0.6384, Recall: 0.5346, F1-score: 0.4492\n",
      "Hamming Loss: 0.3539\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Body Shaming       0.88      0.57      0.69       296\n",
      " Denigration       0.92      0.31      0.47       404\n",
      "     Flaming       0.24      0.98      0.38       296\n",
      "  Harassment       0.29      0.91      0.44       335\n",
      "     Neutral       0.73      0.24      0.36       592\n",
      "\n",
      "   micro avg       0.37      0.53      0.44      1923\n",
      "   macro avg       0.61      0.60      0.47      1923\n",
      "weighted avg       0.64      0.53      0.45      1923\n",
      " samples avg       0.39      0.46      0.41      1923\n",
      "\n",
      "\n",
      "=== ML-KNN 80:20 Trigram ===\n",
      "Subset Accuracy: 0.2749\n",
      "Precision: 0.7774, Recall: 0.4160, F1-score: 0.3894\n",
      "Hamming Loss: 0.2835\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Body Shaming       0.98      0.45      0.61       296\n",
      " Denigration       0.29      1.00      0.45       404\n",
      "     Flaming       0.93      0.39      0.55       296\n",
      "  Harassment       0.93      0.39      0.54       335\n",
      "     Neutral       0.85      0.04      0.07       592\n",
      "\n",
      "   micro avg       0.44      0.42      0.43      1923\n",
      "   macro avg       0.80      0.45      0.44      1923\n",
      "weighted avg       0.78      0.42      0.39      1923\n",
      " samples avg       0.36      0.34      0.35      1923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, hamming_loss, accuracy_score, classification_report\n",
    "\n",
    "# Pastikan konversi sparse matrix menjadi array biasa jika diperlukan\n",
    "X_train_array = X_train_80_tfidf_uni.toarray()  # Mengonversi sparse matrix menjadi array biasa\n",
    "X_test_array = X_test_20_tfidf_uni.toarray()    # Mengonversi sparse matrix menjadi array biasa\n",
    "\n",
    "# Jika y_train_80 adalah list 1D, ubah menjadi array 2D\n",
    "y_train_array = np.array(y_train_80)\n",
    "\n",
    "# Training ML-KNN pada TF-IDF Unigram 80:20\n",
    "ml_knn.fit(X_train_array, y_train_array)\n",
    "y_pred_ml_knn_20 = ml_knn.predict(X_test_array)\n",
    "\n",
    "# Evaluasi ML-KNN (80:20)\n",
    "print(\"\\n=== ML-KNN 80:20 Unigram ===\")\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_20, y_pred_ml_knn_20, average='weighted', zero_division=0)\n",
    "hamming_loss_ml_knn_20 = hamming_loss(y_test_20, y_pred_ml_knn_20)\n",
    "subset_accuracy = accuracy_score(y_test_20, y_pred_ml_knn_20)\n",
    "print(f\"Subset Accuracy: {subset_accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming_loss_ml_knn_20:.4f}\")\n",
    "print(classification_report(y_test_20, y_pred_ml_knn_20, target_names=mlb.classes_, zero_division=0))\n",
    "\n",
    "X_train_array = X_train_80_tfidf_bi.toarray()  # Mengonversi sparse matrix menjadi array biasa\n",
    "X_test_array = X_test_20_tfidf_bi.toarray()    # Mengonversi sparse matrix menjadi array biasa\n",
    "# Training ML-KNN pada TF-IDF Bigram 80:20\n",
    "ml_knn.fit(X_train_array, y_train_array)\n",
    "y_pred_ml_knn_20 = ml_knn.predict(X_test_array)\n",
    "\n",
    "# Evaluasi ML-KNN (80:20)\n",
    "print(\"\\n=== ML-KNN 80:20 Bigram ===\")\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_20, y_pred_ml_knn_20, average='weighted', zero_division=0)\n",
    "hamming_loss_ml_knn_20 = hamming_loss(y_test_20, y_pred_ml_knn_20)\n",
    "subset_accuracy = accuracy_score(y_test_20, y_pred_ml_knn_20)\n",
    "print(f\"Subset Accuracy: {subset_accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming_loss_ml_knn_20:.4f}\")\n",
    "print(classification_report(y_test_20, y_pred_ml_knn_20, target_names=mlb.classes_, zero_division=0))\n",
    "\n",
    "X_train_array = X_train_80_tfidf_tri.toarray()  # Mengonversi sparse matrix menjadi array triasa\n",
    "X_test_array = X_test_20_tfidf_tri.toarray()    # Mengonversi sparse matrix menjadi arraytrbiasa\n",
    "# Training ML-KNN pada TF-IDF Trigram 80:20\n",
    "ml_knn.fit(X_train_array, y_train_array)\n",
    "y_pred_ml_knn_20 = ml_knn.predict(X_test_array)\n",
    "\n",
    "# Evaluasi ML-KNN (80:20)\n",
    "print(\"\\n=== ML-KNN 80:20 Trigram ===\")\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_20, y_pred_ml_knn_20, average='weighted', zero_division=0)\n",
    "hamming_loss_ml_knn_20 = hamming_loss(y_test_20, y_pred_ml_knn_20)\n",
    "subset_accuracy = accuracy_score(y_test_20, y_pred_ml_knn_20)\n",
    "print(f\"Subset Accuracy: {subset_accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming_loss_ml_knn_20:.4f}\")\n",
    "print(classification_report(y_test_20, y_pred_ml_knn_20, target_names=mlb.classes_, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c84a8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ML-KNN 70:30 Unigram ===\n",
      "Subset Accuracy: 0.4750\n",
      "Precision: 0.6749, Recall: 0.6277, F1-score: 0.5888\n",
      "Hamming Loss: 0.2104\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Body Shaming       0.75      0.76      0.75       445\n",
      " Denigration       0.40      0.92      0.56       607\n",
      "     Flaming       0.69      0.70      0.70       443\n",
      "  Harassment       0.72      0.73      0.72       503\n",
      "     Neutral       0.79      0.26      0.40       887\n",
      "\n",
      "   micro avg       0.58      0.63      0.60      2885\n",
      "   macro avg       0.67      0.68      0.63      2885\n",
      "weighted avg       0.67      0.63      0.59      2885\n",
      " samples avg       0.53      0.55      0.53      2885\n",
      "\n",
      "\n",
      "=== ML-KNN 70:30 Bigram ===\n",
      "Subset Accuracy: 0.6060\n",
      "Precision: 0.7647, Recall: 0.6343, F1-score: 0.6491\n",
      "Hamming Loss: 0.1729\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Body Shaming       0.88      0.56      0.68       445\n",
      " Denigration       0.86      0.49      0.62       607\n",
      "     Flaming       0.92      0.51      0.66       443\n",
      "  Harassment       0.84      0.49      0.61       503\n",
      "     Neutral       0.52      0.92      0.66       887\n",
      "\n",
      "   micro avg       0.67      0.63      0.65      2885\n",
      "   macro avg       0.80      0.59      0.65      2885\n",
      "weighted avg       0.76      0.63      0.65      2885\n",
      " samples avg       0.62      0.62      0.62      2885\n",
      "\n",
      "\n",
      "=== ML-KNN 70:30 Trigram ===\n",
      "Subset Accuracy: 0.5644\n",
      "Precision: 0.8032, Recall: 0.5646, F1-score: 0.5735\n",
      "Hamming Loss: 0.1981\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Body Shaming       0.98      0.44      0.61       445\n",
      " Denigration       0.93      0.34      0.50       607\n",
      "     Flaming       0.95      0.38      0.54       443\n",
      "  Harassment       0.93      0.38      0.54       503\n",
      "     Neutral       0.48      0.98      0.64       887\n",
      "\n",
      "   micro avg       0.62      0.56      0.59      2885\n",
      "   macro avg       0.85      0.50      0.57      2885\n",
      "weighted avg       0.80      0.56      0.57      2885\n",
      " samples avg       0.57      0.57      0.57      2885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, hamming_loss, accuracy_score, classification_report\n",
    "\n",
    "# Pastikan konversi sparse matrix menjadi array biasa jika diperlukan\n",
    "X_train_array = X_train_70_tfidf_uni.toarray()  # Mengonversi sparse matrix menjadi array biasa\n",
    "X_test_array = X_test_30_tfidf_uni.toarray()    # Mengonversi sparse matrix menjadi array biasa\n",
    "\n",
    "# Jika y_train_70 adalah list 1D, ubah menjadi array 2D\n",
    "y_train_array = np.array(y_train_70)\n",
    "\n",
    "# Training ML-KNN pada TF-IDF Unigram 70:30\n",
    "ml_knn.fit(X_train_array, y_train_array)\n",
    "y_pred_ml_knn_30 = ml_knn.predict(X_test_array)\n",
    "\n",
    "# Evaluasi ML-KNN (70:30)\n",
    "print(\"\\n=== ML-KNN 70:30 Unigram ===\")\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_30, y_pred_ml_knn_30, average='weighted', zero_division=0)\n",
    "hamming_loss_ml_knn_30 = hamming_loss(y_test_30, y_pred_ml_knn_30)\n",
    "subset_accuracy = accuracy_score(y_test_30, y_pred_ml_knn_30)\n",
    "print(f\"Subset Accuracy: {subset_accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming_loss_ml_knn_30:.4f}\")\n",
    "print(classification_report(y_test_30, y_pred_ml_knn_30, target_names=mlb.classes_, zero_division=0))\n",
    "\n",
    "X_train_array = X_train_70_tfidf_bi.toarray()  # Mengonversi sparse matrix menjadi array biasa\n",
    "X_test_array = X_test_30_tfidf_bi.toarray()    # Mengonversi sparse matrix menjadi array biasa\n",
    "# Training ML-KNN pada TF-IDF Bigram 70:30\n",
    "ml_knn.fit(X_train_array, y_train_array)\n",
    "y_pred_ml_knn_30 = ml_knn.predict(X_test_array)\n",
    "\n",
    "# Evaluasi ML-KNN (70:30)\n",
    "print(\"\\n=== ML-KNN 70:30 Bigram ===\")\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_30, y_pred_ml_knn_30, average='weighted', zero_division=0)\n",
    "hamming_loss_ml_knn_30 = hamming_loss(y_test_30, y_pred_ml_knn_30)\n",
    "subset_accuracy = accuracy_score(y_test_30, y_pred_ml_knn_30)\n",
    "print(f\"Subset Accuracy: {subset_accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming_loss_ml_knn_30:.4f}\")\n",
    "print(classification_report(y_test_30, y_pred_ml_knn_30, target_names=mlb.classes_, zero_division=0))\n",
    "\n",
    "X_train_array = X_train_70_tfidf_tri.toarray()  # Mengonversi sparse matrix menjadi array triasa\n",
    "X_test_array = X_test_30_tfidf_tri.toarray()    # Mengonversi sparse matrix menjadi arraytrbiasa\n",
    "# Training ML-KNN pada TF-IDF Trigram 70:30\n",
    "ml_knn.fit(X_train_array, y_train_array)\n",
    "y_pred_ml_knn_30 = ml_knn.predict(X_test_array)\n",
    "\n",
    "# Evaluasi ML-KNN (70:30)\n",
    "print(\"\\n=== ML-KNN 70:30 Trigram ===\")\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_30, y_pred_ml_knn_30, average='weighted', zero_division=0)\n",
    "hamming_loss_ml_knn_30 = hamming_loss(y_test_30, y_pred_ml_knn_30)\n",
    "subset_accuracy = accuracy_score(y_test_30, y_pred_ml_knn_30)\n",
    "print(f\"Subset Accuracy: {subset_accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming_loss_ml_knn_30:.4f}\")\n",
    "print(classification_report(y_test_30, y_pred_ml_knn_30, target_names=mlb.classes_, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea99551-5fbb-4cc3-b520-049390125527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8f13c6-3af8-4197-8985-918bbb0dea5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f1499-f89f-421a-b1ab-65ad93648b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "13e75b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Body Shaming    84\n",
       "Denigration     93\n",
       "Flaming         92\n",
       "Harassment      97\n",
       "Neutral         64\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_20.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5ac506d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape y_test_array: (357, 5)\n",
      "Shape y_pred_array: (357, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pastikan hasil prediksi berbentuk array 2D\n",
    "y_pred_array = y_pred_ml_knn_20.toarray()\n",
    "y_test_array = np.array(y_test_20)\n",
    "\n",
    "# Buat DataFrame untuk melihat perbandingan label asli dan prediksi\n",
    "# df_comparison = pd.DataFrame({\n",
    "#     \"y_test\": [list(row) for row in y_test_array],   # Label asli\n",
    "#     \"y_pred\": [list(row) for row in y_pred_array]    # Prediksi model\n",
    "# })\n",
    "\n",
    "# Menampilkan beberapa sampel\n",
    "# df_comparison.head(10)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"Shape y_test_array:\", np.shape(y_test_array))\n",
    "print(\"Shape y_pred_array:\", np.shape(y_pred_array))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "676d5aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Body Shaming\n",
      "  TP: 42\n",
      "  TN: 266\n",
      "  FP: 7\n",
      "  FN: 42\n",
      "\n",
      "Label: Denigration\n",
      "  TP: 84\n",
      "  TN: 121\n",
      "  FP: 143\n",
      "  FN: 9\n",
      "\n",
      "Label: Flaming\n",
      "  TP: 38\n",
      "  TN: 249\n",
      "  FP: 16\n",
      "  FN: 54\n",
      "\n",
      "Label: Harassment\n",
      "  TP: 22\n",
      "  TN: 259\n",
      "  FP: 1\n",
      "  FN: 75\n",
      "\n",
      "Label: Neutral\n",
      "  TP: 25\n",
      "  TN: 267\n",
      "  FP: 26\n",
      "  FN: 39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TP, TN, FP, FN untuk setiap label\n",
    "TP = np.sum((y_pred_array == 1) & (y_test_array == 1), axis=0)\n",
    "TN = np.sum((y_pred_array == 0) & (y_test_array == 0), axis=0)\n",
    "FP = np.sum((y_pred_array == 1) & (y_test_array == 0), axis=0)\n",
    "FN = np.sum((y_pred_array == 0) & (y_test_array == 1), axis=0)\n",
    "\n",
    "# Menampilkan hasil untuk setiap label\n",
    "labels = [\"Body Shaming\", \"Denigration\", \"Flaming\", \"Harassment\", \"Neutral\"]\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"  TP: {TP[i]}\")\n",
    "    print(f\"  TN: {TN[i]}\")\n",
    "    print(f\"  FP: {FP[i]}\")\n",
    "    print(f\"  FN: {FN[i]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135cbd1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c3078d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a9a24e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b5e102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d4ccba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17bd3544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deep_translator in c:\\users\\towik\\anaconda3\\lib\\site-packages (1.11.4)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in c:\\users\\towik\\anaconda3\\lib\\site-packages (from deep_translator) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in c:\\users\\towik\\anaconda3\\lib\\site-packages (from deep_translator) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\towik\\anaconda3\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\towik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\towik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\towik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\towik\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install deep_translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af1a84c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MUSISI KEPINGIN DI NOTICEðŸ˜‚ðŸ˜‚ðŸ˜‚.. gw aje gak tau ...</td>\n",
       "      <td>[Denigration, Flaming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ga perlu cantik yg penting gatel+ga tau diriðŸ™‚</td>\n",
       "      <td>[Denigration, Flaming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>biar dapetnya cowo orang hrs gatel dulu ga siðŸ¤ªðŸ¤ª</td>\n",
       "      <td>[Denigration, Flaming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PHO njir, gatellll melebihi ulet bulu</td>\n",
       "      <td>[Denigration, Flaming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Foto bareng MenteriâŒ Foto bareng Koruptor âœ…</td>\n",
       "      <td>[Denigration, Flaming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>Baru saja dirilis sebuah lagu baru, lagunya te...</td>\n",
       "      <td>[Denigration, Flaming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>Kenapa kau tidak mengakui saja kalau kau telah...</td>\n",
       "      <td>[Denigration, Flaming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Bukti SDM Indonesia Rendah, SDM Indonesia tida...</td>\n",
       "      <td>[Denigration, Flaming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Pho Anjir Gangotak masih tergoda sama cowok, lol</td>\n",
       "      <td>[Denigration, Flaming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Aku berpura-pura menjelaskannya, tapi aku tida...</td>\n",
       "      <td>[Denigration, Flaming]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text                   label\n",
       "0    MUSISI KEPINGIN DI NOTICEðŸ˜‚ðŸ˜‚ðŸ˜‚.. gw aje gak tau ...  [Denigration, Flaming]\n",
       "1        ga perlu cantik yg penting gatel+ga tau diriðŸ™‚  [Denigration, Flaming]\n",
       "2      biar dapetnya cowo orang hrs gatel dulu ga siðŸ¤ªðŸ¤ª  [Denigration, Flaming]\n",
       "3                PHO njir, gatellll melebihi ulet bulu  [Denigration, Flaming]\n",
       "4          Foto bareng MenteriâŒ Foto bareng Koruptor âœ…  [Denigration, Flaming]\n",
       "..                                                 ...                     ...\n",
       "235  Baru saja dirilis sebuah lagu baru, lagunya te...  [Denigration, Flaming]\n",
       "236  Kenapa kau tidak mengakui saja kalau kau telah...  [Denigration, Flaming]\n",
       "237  Bukti SDM Indonesia Rendah, SDM Indonesia tida...  [Denigration, Flaming]\n",
       "238   Pho Anjir Gangotak masih tergoda sama cowok, lol  [Denigration, Flaming]\n",
       "239  Aku berpura-pura menjelaskannya, tapi aku tida...  [Denigration, Flaming]\n",
       "\n",
       "[240 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "import time\n",
    "\n",
    "# Ambil hanya data dengan label [Denigration, Flaming]\n",
    "df_filtered = df[df['label'].apply(str) == \"['Denigration', 'Flaming']\"]\n",
    "\n",
    "# Fungsi untuk melakukan back translation\n",
    "def back_translate(text, source_lang, target_lang):\n",
    "    try:\n",
    "        translated = GoogleTranslator(source=source_lang, target=target_lang).translate(text)\n",
    "        back_translated = GoogleTranslator(source=target_lang, target=source_lang).translate(translated)\n",
    "        return back_translated\n",
    "    except Exception as e:\n",
    "        print(f\"Error saat menerjemahkan: {e}\")\n",
    "        return text  # Jika gagal, kembalikan teks aslinya\n",
    "\n",
    "# Back translation tahap pertama (Indonesia â†’ Inggris â†’ Indonesia)\n",
    "translated_texts_1 = [back_translate(text, 'id', 'en') for text in df_filtered['text']]\n",
    "time.sleep(1)  # Hindari rate limit\n",
    "\n",
    "# Gabungkan data asli dengan hasil translasi pertama\n",
    "combined_texts_1 = df_filtered['text'].tolist() + translated_texts_1\n",
    "\n",
    "# Lakukan back translation tahap kedua (Indonesia â†’ Jerman â†’ Indonesia)\n",
    "translated_texts_2 = [back_translate(text, 'id', 'de') for text in combined_texts_1]\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "# Gabungkan data asli + hasil translasi tahap 1 + hasil translasi tahap 2\n",
    "final_augmented_texts = list(df_filtered['text']) + translated_texts_1 + translated_texts_2\n",
    "\n",
    "# Buat dataframe baru untuk menyimpan data augmentasi\n",
    "df_augmented = pd.DataFrame({'text': final_augmented_texts, 'label': '[Denigration, Flaming]'})\n",
    "\n",
    "# Simpan ke CSV (opsional)\n",
    "df_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4709caa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augmented.to_csv(\"augmented_deniflam.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fb82ccb-532a-4cd7-a5b0-ea370e02a1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bohai.banget badannya Sitha..pantesan Bastian ...</td>\n",
       "      <td>[Body Shaming, Denigration]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baru tau tante ini gara2 dia ngerebut si riski</td>\n",
       "      <td>[Body Shaming, Denigration]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RIZKY LEPAS BERLIAN DEMI BATU KERIKILðŸ¤­ðŸ¤­ðŸ˜‚TAPI G...</td>\n",
       "      <td>[Body Shaming, Denigration]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kurus kering kurang makan ya.</td>\n",
       "      <td>[Body Shaming, Denigration]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>duh makanya jan jadi slingkuhan mba,kan ga bis...</td>\n",
       "      <td>[Body Shaming, Denigration]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Anda tidak dapat memposting jika pacar Anda di...</td>\n",
       "      <td>[Body Shaming, Denigration]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Dari Syifa Hadju sampai Dosma Gila jaraknya ba...</td>\n",
       "      <td>[Body Shaming, Denigration]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Ya, dibandingkan dengan Syifa Dosma dia benar-...</td>\n",
       "      <td>[Body Shaming, Denigration]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>Mengapa sahabat sejati lebih indah dari pada k...</td>\n",
       "      <td>[Body Shaming, Denigration]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Rizky terobsesi sama dia, mana mungkin dia sel...</td>\n",
       "      <td>[Body Shaming, Denigration]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>232 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    Bohai.banget badannya Sitha..pantesan Bastian ...   \n",
       "1       Baru tau tante ini gara2 dia ngerebut si riski   \n",
       "2    RIZKY LEPAS BERLIAN DEMI BATU KERIKILðŸ¤­ðŸ¤­ðŸ˜‚TAPI G...   \n",
       "3                        Kurus kering kurang makan ya.   \n",
       "4    duh makanya jan jadi slingkuhan mba,kan ga bis...   \n",
       "..                                                 ...   \n",
       "227  Anda tidak dapat memposting jika pacar Anda di...   \n",
       "228  Dari Syifa Hadju sampai Dosma Gila jaraknya ba...   \n",
       "229  Ya, dibandingkan dengan Syifa Dosma dia benar-...   \n",
       "230  Mengapa sahabat sejati lebih indah dari pada k...   \n",
       "231  Rizky terobsesi sama dia, mana mungkin dia sel...   \n",
       "\n",
       "                           label  \n",
       "0    [Body Shaming, Denigration]  \n",
       "1    [Body Shaming, Denigration]  \n",
       "2    [Body Shaming, Denigration]  \n",
       "3    [Body Shaming, Denigration]  \n",
       "4    [Body Shaming, Denigration]  \n",
       "..                           ...  \n",
       "227  [Body Shaming, Denigration]  \n",
       "228  [Body Shaming, Denigration]  \n",
       "229  [Body Shaming, Denigration]  \n",
       "230  [Body Shaming, Denigration]  \n",
       "231  [Body Shaming, Denigration]  \n",
       "\n",
       "[232 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "import time\n",
    "\n",
    "# Ambil hanya data dengan label [Denigration, Flaming]\n",
    "df_filtered = df[df['label'].apply(str) == \"['Body Shaming', 'Denigration']\"]\n",
    "\n",
    "# Fungsi untuk melakukan back translation\n",
    "def back_translate(text, source_lang, target_lang):\n",
    "    try:\n",
    "        translated = GoogleTranslator(source=source_lang, target=target_lang).translate(text)\n",
    "        back_translated = GoogleTranslator(source=target_lang, target=source_lang).translate(translated)\n",
    "        return back_translated\n",
    "    except Exception as e:\n",
    "        print(f\"Error saat menerjemahkan: {e}\")\n",
    "        return text  # Jika gagal, kembalikan teks aslinya\n",
    "\n",
    "# Back translation tahap pertama (Indonesia â†’ Inggris â†’ Indonesia)\n",
    "translated_texts_1 = [back_translate(text, 'id', 'en') for text in df_filtered['text']]\n",
    "time.sleep(1)  # Hindari rate limit\n",
    "\n",
    "# Gabungkan data asli dengan hasil translasi pertama\n",
    "combined_texts_1 = df_filtered['text'].tolist() + translated_texts_1\n",
    "\n",
    "# Lakukan back translation tahap kedua (Indonesia â†’ Jerman â†’ Indonesia)\n",
    "translated_texts_2 = [back_translate(text, 'id', 'de') for text in combined_texts_1]\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "# Gabungkan data asli + hasil translasi tahap 1 + hasil translasi tahap 2\n",
    "final_augmented_texts = list(df_filtered['text']) + translated_texts_1 + translated_texts_2\n",
    "\n",
    "# Buat dataframe baru untuk menyimpan data augmentasi\n",
    "df_augmented = pd.DataFrame({'text': final_augmented_texts, 'label': '[Body Shaming, Denigration]'})\n",
    "\n",
    "# Simpan ke CSV (opsional)\n",
    "df_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "840136fc-0d8e-45ff-a30c-12786def0a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augmented.to_csv(\"augmented_bodydeni.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bc169c7-fcff-41a5-b2e7-f2067e01ac76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mekanik bastian op sih</td>\n",
       "      <td>[Denigration, Harassment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cancel culture aja dua manusia menggelikan ini...</td>\n",
       "      <td>[Denigration, Harassment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hanya Bastian yg tau rasanya</td>\n",
       "      <td>[Denigration, Harassment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>duit haram hasil pencucian uang gak bakal bert...</td>\n",
       "      <td>[Denigration, Harassment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jelass si kawan dah mati rasa.. tiapp hari ðŸ”¥ ðŸ˜‚</td>\n",
       "      <td>[Denigration, Harassment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>Vulgar banget di depan Bastian ðŸ˜‚ðŸ˜‚</td>\n",
       "      <td>[Denigration, Harassment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Tentu saja tidak mungkin ðŸ˜‚</td>\n",
       "      <td>[Denigration, Harassment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Untung ane gak kenal orang ini, gak kenal lagu...</td>\n",
       "      <td>[Denigration, Harassment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Seperti biasa, cara murah untuk menaiki tangga...</td>\n",
       "      <td>[Denigration, Harassment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Anda berpura-pura saleh tapi ketahuan berbuat ...</td>\n",
       "      <td>[Denigration, Harassment]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0                               mekanik bastian op sih   \n",
       "1    Cancel culture aja dua manusia menggelikan ini...   \n",
       "2                         Hanya Bastian yg tau rasanya   \n",
       "3    duit haram hasil pencucian uang gak bakal bert...   \n",
       "4       Jelass si kawan dah mati rasa.. tiapp hari ðŸ”¥ ðŸ˜‚   \n",
       "..                                                 ...   \n",
       "207                  Vulgar banget di depan Bastian ðŸ˜‚ðŸ˜‚   \n",
       "208                         Tentu saja tidak mungkin ðŸ˜‚   \n",
       "209  Untung ane gak kenal orang ini, gak kenal lagu...   \n",
       "210  Seperti biasa, cara murah untuk menaiki tangga...   \n",
       "211  Anda berpura-pura saleh tapi ketahuan berbuat ...   \n",
       "\n",
       "                         label  \n",
       "0    [Denigration, Harassment]  \n",
       "1    [Denigration, Harassment]  \n",
       "2    [Denigration, Harassment]  \n",
       "3    [Denigration, Harassment]  \n",
       "4    [Denigration, Harassment]  \n",
       "..                         ...  \n",
       "207  [Denigration, Harassment]  \n",
       "208  [Denigration, Harassment]  \n",
       "209  [Denigration, Harassment]  \n",
       "210  [Denigration, Harassment]  \n",
       "211  [Denigration, Harassment]  \n",
       "\n",
       "[212 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "import time\n",
    "\n",
    "# Ambil hanya data dengan label [Denigration, Flaming]\n",
    "df_filtered = df[df['label'].apply(str) == \"['Denigration', 'Harassment']\"]\n",
    "\n",
    "# Fungsi untuk melakukan back translation\n",
    "def back_translate(text, source_lang, target_lang):\n",
    "    try:\n",
    "        translated = GoogleTranslator(source=source_lang, target=target_lang).translate(text)\n",
    "        back_translated = GoogleTranslator(source=target_lang, target=source_lang).translate(translated)\n",
    "        return back_translated\n",
    "    except Exception as e:\n",
    "        print(f\"Error saat menerjemahkan: {e}\")\n",
    "        return text  # Jika gagal, kembalikan teks aslinya\n",
    "\n",
    "# Back translation tahap pertama (Indonesia â†’ Inggris â†’ Indonesia)\n",
    "translated_texts_1 = [back_translate(text, 'id', 'en') for text in df_filtered['text']]\n",
    "time.sleep(1)  # Hindari rate limit\n",
    "\n",
    "# Gabungkan data asli dengan hasil translasi pertama\n",
    "combined_texts_1 = df_filtered['text'].tolist() + translated_texts_1\n",
    "\n",
    "# Lakukan back translation tahap kedua (Indonesia â†’ Jerman â†’ Indonesia)\n",
    "translated_texts_2 = [back_translate(text, 'id', 'de') for text in combined_texts_1]\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "# Gabungkan data asli + hasil translasi tahap 1 + hasil translasi tahap 2\n",
    "final_augmented_texts = list(df_filtered['text']) + translated_texts_1 + translated_texts_2\n",
    "\n",
    "# Buat dataframe baru untuk menyimpan data augmentasi\n",
    "df_augmented = pd.DataFrame({'text': final_augmented_texts, 'label': '[Denigration, Harassment]'})\n",
    "\n",
    "# Simpan ke CSV (opsional)\n",
    "df_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e21b0402-d5eb-48fb-ad8e-6f05d0eb4e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augmented.to_csv(\"augmented_denihara.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f3a4e72-1de9-4ab0-b640-4bb012bd4214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>muka sama lagu gaseberapa sok iyeeðŸ¤£ðŸ¤£ gasudi gu...</td>\n",
       "      <td>[Body Shaming, Harassment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Muka kaya jalan berlubang, badan kaya papan, t...</td>\n",
       "      <td>[Body Shaming, Harassment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bongsor sama badan melar karena udah gak pw/tu...</td>\n",
       "      <td>[Body Shaming, Harassment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Badan lo kaya galon isi angin doang, tapi gaya...</td>\n",
       "      <td>[Body Shaming, Harassment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Buset lo gede banget, dikira tukang pukul kafe...</td>\n",
       "      <td>[Body Shaming, Harassment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Lagunya jelek, penyanyinya juga jelek, aduh, g...</td>\n",
       "      <td>[Body Shaming, Harassment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Ada sesuatu yang besar, tapi tidak ada tekad</td>\n",
       "      <td>[Body Shaming, Harassment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Mereka seperti selembar kayu lapis dan tetap m...</td>\n",
       "      <td>[Body Shaming, Harassment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Kamu sudah tua, wajahmu keriput, tubuhmu seper...</td>\n",
       "      <td>[Body Shaming, Harassment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Dimana lehermu? Apakah ditutupi lipatan lemak?...</td>\n",
       "      <td>[Body Shaming, Harassment]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    muka sama lagu gaseberapa sok iyeeðŸ¤£ðŸ¤£ gasudi gu...   \n",
       "1    Muka kaya jalan berlubang, badan kaya papan, t...   \n",
       "2    Bongsor sama badan melar karena udah gak pw/tu...   \n",
       "3    Badan lo kaya galon isi angin doang, tapi gaya...   \n",
       "4    Buset lo gede banget, dikira tukang pukul kafe...   \n",
       "..                                                 ...   \n",
       "183  Lagunya jelek, penyanyinya juga jelek, aduh, g...   \n",
       "184       Ada sesuatu yang besar, tapi tidak ada tekad   \n",
       "185  Mereka seperti selembar kayu lapis dan tetap m...   \n",
       "186  Kamu sudah tua, wajahmu keriput, tubuhmu seper...   \n",
       "187  Dimana lehermu? Apakah ditutupi lipatan lemak?...   \n",
       "\n",
       "                          label  \n",
       "0    [Body Shaming, Harassment]  \n",
       "1    [Body Shaming, Harassment]  \n",
       "2    [Body Shaming, Harassment]  \n",
       "3    [Body Shaming, Harassment]  \n",
       "4    [Body Shaming, Harassment]  \n",
       "..                          ...  \n",
       "183  [Body Shaming, Harassment]  \n",
       "184  [Body Shaming, Harassment]  \n",
       "185  [Body Shaming, Harassment]  \n",
       "186  [Body Shaming, Harassment]  \n",
       "187  [Body Shaming, Harassment]  \n",
       "\n",
       "[188 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "import time\n",
    "\n",
    "# Ambil hanya data dengan label [Denigration, Flaming]\n",
    "df_filtered = df[df['label'].apply(str) == \"['Body Shaming', 'Harassment']\"]\n",
    "\n",
    "# Fungsi untuk melakukan back translation\n",
    "def back_translate(text, source_lang, target_lang):\n",
    "    try:\n",
    "        translated = GoogleTranslator(source=source_lang, target=target_lang).translate(text)\n",
    "        back_translated = GoogleTranslator(source=target_lang, target=source_lang).translate(translated)\n",
    "        return back_translated\n",
    "    except Exception as e:\n",
    "        print(f\"Error saat menerjemahkan: {e}\")\n",
    "        return text  # Jika gagal, kembalikan teks aslinya\n",
    "\n",
    "# Back translation tahap pertama (Indonesia â†’ Inggris â†’ Indonesia)\n",
    "translated_texts_1 = [back_translate(text, 'id', 'en') for text in df_filtered['text']]\n",
    "time.sleep(1)  # Hindari rate limit\n",
    "\n",
    "# Gabungkan data asli dengan hasil translasi pertama\n",
    "combined_texts_1 = df_filtered['text'].tolist() + translated_texts_1\n",
    "\n",
    "# Lakukan back translation tahap kedua (Indonesia â†’ Jerman â†’ Indonesia)\n",
    "translated_texts_2 = [back_translate(text, 'id', 'de') for text in combined_texts_1]\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "# Gabungkan data asli + hasil translasi tahap 1 + hasil translasi tahap 2\n",
    "final_augmented_texts = list(df_filtered['text']) + translated_texts_1 + translated_texts_2\n",
    "\n",
    "# Buat dataframe baru untuk menyimpan data augmentasi\n",
    "df_augmented = pd.DataFrame({'text': final_augmented_texts, 'label': '[Body Shaming, Harassment]'})\n",
    "\n",
    "# Simpan ke CSV (opsional)\n",
    "df_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8bb8051-2910-45ab-ba8a-f36a1f3ad2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augmented.to_csv(\"augmented_bodyhara.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f030329-ea69-4436-b18e-51f08c11bca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anjir fllwrrrrrrr nyaa jauh bgt sama cipaaaé¦ƒæ§¶é¦ƒ...</td>\n",
       "      <td>[Body Shaming, Flaming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>badan lo gede doang, otak sama mental nol besar!</td>\n",
       "      <td>[Body Shaming, Flaming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pipi lo segede drum band, tapi mulut ga pernah...</td>\n",
       "      <td>[Body Shaming, Flaming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Muka lo lebar banget njir, udah gitu kelakuan ...</td>\n",
       "      <td>[Body Shaming, Flaming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Muka kaya bajingan gini sokè™an gak mau di dnge...</td>\n",
       "      <td>[Body Shaming, Flaming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Mukanya kaya kadal, berisik banget!!! Ada lagu...</td>\n",
       "      <td>[Body Shaming, Flaming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Ya ampun, minimal KENALI DIRI SENDIRI, muka lu...</td>\n",
       "      <td>[Body Shaming, Flaming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Tipis seperti tiang listrik, tapi sangat bersih</td>\n",
       "      <td>[Body Shaming, Flaming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Ingat, kumis saya terlihat sekuat bulu, keren ...</td>\n",
       "      <td>[Body Shaming, Flaming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Mata orang ini katarak, kan?</td>\n",
       "      <td>[Body Shaming, Flaming]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    anjir fllwrrrrrrr nyaa jauh bgt sama cipaaaé¦ƒæ§¶é¦ƒ...   \n",
       "1     badan lo gede doang, otak sama mental nol besar!   \n",
       "2    Pipi lo segede drum band, tapi mulut ga pernah...   \n",
       "3    Muka lo lebar banget njir, udah gitu kelakuan ...   \n",
       "4    Muka kaya bajingan gini sokè™an gak mau di dnge...   \n",
       "..                                                 ...   \n",
       "175  Mukanya kaya kadal, berisik banget!!! Ada lagu...   \n",
       "176  Ya ampun, minimal KENALI DIRI SENDIRI, muka lu...   \n",
       "177    Tipis seperti tiang listrik, tapi sangat bersih   \n",
       "178  Ingat, kumis saya terlihat sekuat bulu, keren ...   \n",
       "179                       Mata orang ini katarak, kan?   \n",
       "\n",
       "                       label  \n",
       "0    [Body Shaming, Flaming]  \n",
       "1    [Body Shaming, Flaming]  \n",
       "2    [Body Shaming, Flaming]  \n",
       "3    [Body Shaming, Flaming]  \n",
       "4    [Body Shaming, Flaming]  \n",
       "..                       ...  \n",
       "175  [Body Shaming, Flaming]  \n",
       "176  [Body Shaming, Flaming]  \n",
       "177  [Body Shaming, Flaming]  \n",
       "178  [Body Shaming, Flaming]  \n",
       "179  [Body Shaming, Flaming]  \n",
       "\n",
       "[180 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "import time\n",
    "\n",
    "# Ambil hanya data dengan label [Denigration, Flaming]\n",
    "df_filtered = df[df['label'].apply(str) == \"['Body Shaming', 'Flaming']\"]\n",
    "\n",
    "# Fungsi untuk melakukan back translation\n",
    "def back_translate(text, source_lang, target_lang):\n",
    "    try:\n",
    "        translated = GoogleTranslator(source=source_lang, target=target_lang).translate(text)\n",
    "        back_translated = GoogleTranslator(source=target_lang, target=source_lang).translate(translated)\n",
    "        return back_translated\n",
    "    except Exception as e:\n",
    "        print(f\"Error saat menerjemahkan: {e}\")\n",
    "        return text  # Jika gagal, kembalikan teks aslinya\n",
    "\n",
    "# Back translation tahap pertama (Indonesia â†’ Inggris â†’ Indonesia)\n",
    "translated_texts_1 = [back_translate(text, 'id', 'en') for text in df_filtered['text']]\n",
    "time.sleep(1)  # Hindari rate limit\n",
    "\n",
    "# Gabungkan data asli dengan hasil translasi pertama\n",
    "combined_texts_1 = df_filtered['text'].tolist() + translated_texts_1\n",
    "\n",
    "# Lakukan back translation tahap kedua (Indonesia â†’ Jerman â†’ Indonesia)\n",
    "translated_texts_2 = [back_translate(text, 'id', 'de') for text in combined_texts_1]\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "# Gabungkan data asli + hasil translasi tahap 1 + hasil translasi tahap 2\n",
    "final_augmented_texts = list(df_filtered['text']) + translated_texts_1 + translated_texts_2\n",
    "\n",
    "# Buat dataframe baru untuk menyimpan data augmentasi\n",
    "df_augmented = pd.DataFrame({'text': final_augmented_texts, 'label': '[Body Shaming, Flaming]'})\n",
    "\n",
    "# Simpan ke CSV (opsional)\n",
    "df_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06a94010-cd30-48ed-9e31-6a903d05e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augmented.to_csv(\"augmented_bodyflam.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52a3a2da-1433-4307-9959-881909960424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         label\n",
      "0  ['Body Shaming', 'Flaming']\n",
      "1  ['Body Shaming', 'Flaming']\n",
      "2  ['Body Shaming', 'Flaming']\n",
      "3  ['Body Shaming', 'Flaming']\n",
      "4  ['Body Shaming', 'Flaming']\n",
      "                            label\n",
      "0  ['Body Shaming', 'Harassment']\n",
      "1  ['Body Shaming', 'Harassment']\n",
      "2  ['Body Shaming', 'Harassment']\n",
      "3  ['Body Shaming', 'Harassment']\n",
      "4  ['Body Shaming', 'Harassment']\n",
      "                        label\n",
      "0  ['Denigration', 'Flaming']\n",
      "1  ['Denigration', 'Flaming']\n",
      "2  ['Denigration', 'Flaming']\n",
      "3  ['Denigration', 'Flaming']\n",
      "4  ['Denigration', 'Flaming']\n",
      "                           label\n",
      "0  ['Denigration', 'Harassment']\n",
      "1  ['Denigration', 'Harassment']\n",
      "2  ['Denigration', 'Harassment']\n",
      "3  ['Denigration', 'Harassment']\n",
      "4  ['Denigration', 'Harassment']\n",
      "                             label\n",
      "0  ['Body Shaming', 'Denigration']\n",
      "1  ['Body Shaming', 'Denigration']\n",
      "2  ['Body Shaming', 'Denigration']\n",
      "3  ['Body Shaming', 'Denigration']\n",
      "4  ['Body Shaming', 'Denigration']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(r'C:\\Users\\towik\\augmented_data.csv')\n",
    "df2 = pd.read_csv(r'C:\\Users\\towik\\augmented_bodyflam.csv')\n",
    "df3 = pd.read_csv(r'C:\\Users\\towik\\augmented_bodyhara.csv')\n",
    "df4 = pd.read_csv(r'C:\\Users\\towik\\augmented_deniflam.csv')\n",
    "df5 = pd.read_csv(r'C:\\Users\\towik\\augmented_denihara.csv')\n",
    "df6 = pd.read_csv(r'C:\\Users\\towik\\augmented_bodydeni.csv')\n",
    "\n",
    "df2['label'] = df2['label'].apply(lambda x: \"['Body Shaming', 'Flaming']\")\n",
    "df3['label'] = df3['label'].apply(lambda x: \"['Body Shaming', 'Harassment']\")\n",
    "df4['label'] = df4['label'].apply(lambda x: \"['Denigration', 'Flaming']\")\n",
    "df5['label'] = df5['label'].apply(lambda x: \"['Denigration', 'Harassment']\")\n",
    "df6['label'] = df6['label'].apply(lambda x: \"['Body Shaming', 'Denigration']\")\n",
    "\n",
    "# Cek hasil\n",
    "print(df2[['label']].head())\n",
    "print(df3[['label']].head())\n",
    "print(df4[['label']].head())\n",
    "print(df5[['label']].head())\n",
    "print(df6[['label']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d04b30db-96da-4afa-acb5-444c8b4bfeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df1, df2, df3, df4, df5, df6], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9f91cf2-1fce-42c9-91dd-567b085a96dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "['Neutral']                        321\n",
       "['Denigration', 'Flaming']         300\n",
       "['Denigration']                    291\n",
       "['Body Shaming', 'Denigration']    290\n",
       "['Harassment']                     282\n",
       "['Body Shaming']                   268\n",
       "['Denigration', 'Harassment']      265\n",
       "['Flaming']                        256\n",
       "['Body Shaming', 'Harassment']     235\n",
       "['Body Shaming', 'Flaming']        225\n",
       "['Flaming', 'Harassment']          202\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined['label'].apply(str).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e489fac5-c394-4945-b64b-3bc2ef37fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[['username', 'text', 'label']].to_csv(\"fixxx_augmented.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "206c09b7-9c92-4adb-96af-a4067ad11812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_comment</th>\n",
       "      <th>Body Shaming</th>\n",
       "      <th>Denigration</th>\n",
       "      <th>Flaming</th>\n",
       "      <th>Harassment</th>\n",
       "      <th>Neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>befrychandra</td>\n",
       "      <td>Lagi banyak pikiran bang... SombongnyaðŸ¤£</td>\n",
       "      <td>['Flaming']</td>\n",
       "      <td>['pikir', 'bang', 'sombong']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abcdefuc3k_</td>\n",
       "      <td>mekanik bastian op sih</td>\n",
       "      <td>['Denigration', 'Harassment']</td>\n",
       "      <td>['mekanik', 'bastian', 'op', 'sih']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vivinagustin__</td>\n",
       "      <td>lagu nggak enak merasaa paling sukses...ðŸ¤£ðŸ¤£ðŸ¤£ðŸ¤£ d...</td>\n",
       "      <td>['Flaming']</td>\n",
       "      <td>['lagu', 'tidak', 'bagus', 'sukses', 'dewa', '...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vviaa.xo</td>\n",
       "      <td>pinggangmu sebesar pahaku kakkk</td>\n",
       "      <td>['Body Shaming']</td>\n",
       "      <td>['pinggang', 'paha', 'kak']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ilhamjhonathan86</td>\n",
       "      <td>This is not nina?</td>\n",
       "      <td>['Neutral']</td>\n",
       "      <td>['bukan', 'nina']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2870</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Anda tidak dapat memposting jika pacar Anda di...</td>\n",
       "      <td>['Body Shaming', 'Denigration']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2871</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Dari Syifa Hadju ke Dosma Gila jaraknya ibarat...</td>\n",
       "      <td>['Body Shaming', 'Denigration']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2872</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Ya, dibandingkan dengan Syifa Dosma dia benar-...</td>\n",
       "      <td>['Body Shaming', 'Denigration']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2873</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Mengapa sahabat sejati lebih cantik dari pada ...</td>\n",
       "      <td>['Body Shaming', 'Denigration']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2874</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Rizky terobsesi sama dia, mana mungkin dia sel...</td>\n",
       "      <td>['Body Shaming', 'Denigration']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2875 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              username                                               text  \\\n",
       "0         befrychandra            Lagi banyak pikiran bang... SombongnyaðŸ¤£   \n",
       "1          abcdefuc3k_                             mekanik bastian op sih   \n",
       "2       vivinagustin__  lagu nggak enak merasaa paling sukses...ðŸ¤£ðŸ¤£ðŸ¤£ðŸ¤£ d...   \n",
       "3             vviaa.xo                    pinggangmu sebesar pahaku kakkk   \n",
       "4     ilhamjhonathan86                                  This is not nina?   \n",
       "...                ...                                                ...   \n",
       "2870               NaN  Anda tidak dapat memposting jika pacar Anda di...   \n",
       "2871               NaN  Dari Syifa Hadju ke Dosma Gila jaraknya ibarat...   \n",
       "2872               NaN  Ya, dibandingkan dengan Syifa Dosma dia benar-...   \n",
       "2873               NaN  Mengapa sahabat sejati lebih cantik dari pada ...   \n",
       "2874               NaN  Rizky terobsesi sama dia, mana mungkin dia sel...   \n",
       "\n",
       "                                label  \\\n",
       "0                         ['Flaming']   \n",
       "1       ['Denigration', 'Harassment']   \n",
       "2                         ['Flaming']   \n",
       "3                    ['Body Shaming']   \n",
       "4                         ['Neutral']   \n",
       "...                               ...   \n",
       "2870  ['Body Shaming', 'Denigration']   \n",
       "2871  ['Body Shaming', 'Denigration']   \n",
       "2872  ['Body Shaming', 'Denigration']   \n",
       "2873  ['Body Shaming', 'Denigration']   \n",
       "2874  ['Body Shaming', 'Denigration']   \n",
       "\n",
       "                                      tokenized_comment  Body Shaming  \\\n",
       "0                          ['pikir', 'bang', 'sombong']           0.0   \n",
       "1                   ['mekanik', 'bastian', 'op', 'sih']           0.0   \n",
       "2     ['lagu', 'tidak', 'bagus', 'sukses', 'dewa', '...           0.0   \n",
       "3                           ['pinggang', 'paha', 'kak']           1.0   \n",
       "4                                     ['bukan', 'nina']           0.0   \n",
       "...                                                 ...           ...   \n",
       "2870                                                NaN           NaN   \n",
       "2871                                                NaN           NaN   \n",
       "2872                                                NaN           NaN   \n",
       "2873                                                NaN           NaN   \n",
       "2874                                                NaN           NaN   \n",
       "\n",
       "      Denigration  Flaming  Harassment  Neutral  \n",
       "0             0.0      1.0         0.0      0.0  \n",
       "1             1.0      0.0         1.0      0.0  \n",
       "2             0.0      1.0         0.0      0.0  \n",
       "3             0.0      0.0         0.0      0.0  \n",
       "4             0.0      0.0         0.0      1.0  \n",
       "...           ...      ...         ...      ...  \n",
       "2870          NaN      NaN         NaN      NaN  \n",
       "2871          NaN      NaN         NaN      NaN  \n",
       "2872          NaN      NaN         NaN      NaN  \n",
       "2873          NaN      NaN         NaN      NaN  \n",
       "2874          NaN      NaN         NaN      NaN  \n",
       "\n",
       "[2875 rows x 9 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1963e8a-10d1-4c5e-9de4-8b8b454114ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2933c170-6ff7-4e54-8691-00d2b526a816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d80e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.adapt import MLkNN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "class CustomMLkNN(MLkNN):\n",
    "    def _compute_cond(self, X, y):\n",
    "        \"\"\"Mengganti pemanggilan NearestNeighbors dengan cara yang benar\"\"\"\n",
    "        self.knn_ = NearestNeighbors(n_neighbors=self.k).fit(X)  # Perbaikan di sini\n",
    "        # Lanjutkan dengan kode lainnya...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
